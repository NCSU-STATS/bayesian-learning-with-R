---
title: "Applied Bayesian Analysis : NCSU ST 540"
subtitle: "Midterm2"
author: "Bruce Campbell"
fontsize: 11pt
output: pdf_document
bibliography: BruceCampbell_ST540_HW_1.bib
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
setwd("c:/e/brucebcampbell-git/bayesian-learning-with-R")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=5)
knitr::opts_chunk$set(fig.width=6)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=38),tidy=TRUE)
library(latex2exp)   
library(pander)
library(ggplot2)
library(GGally)
```

This section is a test section where we generate and fit a vector autoregressive  model - $VAR(1) \in \mathbf{R}^6$ given by 

$$y_{t} = \nu + \rho * y_{t-1} + \epsilon$$ 

$$\epsilon \sim N(0,\Sigma)$$

We use the $y1$ data to calculate a NaN firendly sample covariance and then we find the nearest positive semidefinite matrix to use to generate data for the model. 

#OPENBUGS MODEL

```{r,echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(rstan)
library(bayesplot)
library(shinystan)
library(rstan)

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

#Check C++ compiler in R
Sys.which("g++")
Sys.getenv("PATH")
schools_dat <- list(J = 8, 
                    y = c(28,  8, -3,  7, -1,  1, 18, 12),
                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
fit <- stan(file = 'schools.stan', data = schools_dat,   iter = 1000, chains = 4)


```


```{r}
library(coda)
library(modeest)
library(MASS)
load("E2.RData")
cov.y1  <- cov(Y1, use = "pairwise.complete.obs")
N <- nrow(Y1)
p = 6

n.chains = 2

stacks_dat <- list(Y1=scale(Y1),Y2=as.vector(scale(Y2)),Y3=as.vector(scale(Y3)), p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.20, sigmaY2=0,sigmaY3=0, precision=diag(p),Y1_prec=diag(p) ,Y1_mn=rep(p,0) ) }
samps <- bugs(data = stacks_dat, 
            inits = mlr_inits, 
            parameters.to.save = c("Y1pred","SigmaVAR"), 
            model.file = openbugs_model, 
            codaPkg = TRUE,
            n.chains = n.chains, n.burnin=1000, n.iter = 10000, DIC=F)

out.coda <- read.bugs(samps)

save(out.coda,file = "out.coda_OPENBUGS_VAR1.RData")

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]

Y1.posterior.modes <- list()

for( i in 1:(365*6+36) )
{  
  colname <- colnames(chain)[i]
  if(grepl("Sigma",colname)  )
  {
    print(paste("skipping ",colname,sep=""))
    next
  }
  samples <- chain[,i]
  samplesId <- samples<4
  red <- samples[samplesId]
  Y1.posterior.modes[colname] <-mlv(red)$M
}

theta.map <-  matrix(unlist(Y1.posterior.modes),ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1,na.rm = TRUE)) *  apply(Y1, 2,sd,na.rm = TRUE)

image(Y1, main="Y1")

image(unscaled.theta.map, main="Imputed")

SigmaVAR.posterior.modes <- list()

for( i in 1:(365*6+36) )
{  
  colname <- colnames(chain)[i]
  if(grepl("Sigma",colname) ==FALSE )
  {
    next
  }
  samples <- chain[,i]
  SigmaVAR.posterior.modes[colname] <-mlv(samples)$M
}

Sigma.map <-  matrix(unlist(SigmaVAR.posterior.modes),ncol=6, byrow=FALSE)
image(Sigma.map, main = "MAP Sigma")
image(cov.y1,main = "cov of Y1")

b <- unscaled.theta.map[!is.na(Y1)]
c <- Y1[!is.na(Y1)]

plot(b,c)
write.csv(unscaled.theta.map,file = "unscaled-theta-map.csv")
```


##Generative Model
```{r}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
library(rjags)
library(coda)
library(modeest)
library(MASS)
load("E2.RData")
cov.y1  <- cov(Y1, use = "pairwise.complete.obs")
ggplot(data.frame(Y3=Y3) ,aes(x=1:365, y=Y3)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Y3")
ggplot(data.frame(mean.Y1=rowMeans(Y1,na.rm = TRUE)) ,aes(x=1:365, y=mean.Y1)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Row Means Y1")
N <- nrow(Y1)
p = 6
library(R2OpenBUGS)

openbugs_model <- function(){
  
  for(i in 1:N) {
  Y1pred[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
      for(j in 1:p){
       theta[i,j]<-  x[i,j] #This is where we can add Y2 and Y3 for generative model.
      }
  }
  
  precision[1:p,1:p] ~ dwish(R[,],k)
  
  # Missing data model for x
  for(i in 1:N){
    x[i,1:p]~dmnorm(x_mn[],x_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    x_mn[j]~dnorm(0,0.01)
  }
  x_prec[1:p,1:p]~dwish(R[,],k)
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
}
n.chains = 2

stacks_dat <- list(Y1=scale(Y1),Y2=as.vector(scale(Y2)),Y3=as.vector(scale(Y3)), p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.20, sigmaY2=0,sigmaY3=0, precision=diag(p),Y1_prec=diag(p) ,Y1_mn=rep(p,0) ) }
samps <- bugs(data = stacks_dat, 
            inits = mlr_inits, 
            parameters.to.save = c("Y1pred","SigmaVAR"), 
            model.file = openbugs_model, 
            codaPkg = TRUE,
            n.chains = n.chains, n.burnin=1000, n.iter = 10000, DIC=F)

out.coda <- read.bugs(samps)

save(out.coda,file = "out.coda_OPENBUGS_VAR1.RData")

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]

Y1.posterior.modes <- list()

for( i in 1:(365*6+36) )
{  
  colname <- colnames(chain)[i]
  if(grepl("Sigma",colname)  )
  {
    print(paste("skipping ",colname,sep=""))
    next
  }
  samples <- chain[,i]
  samplesId <- samples<4
  red <- samples[samplesId]
  Y1.posterior.modes[colname] <-mlv(red)$M
}

theta.map <-  matrix(unlist(Y1.posterior.modes),ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1,na.rm = TRUE)) *  apply(Y1, 2,sd,na.rm = TRUE)

image(Y1, main="Y1")

image(unscaled.theta.map, main="Imputed")

SigmaVAR.posterior.modes <- list()

for( i in 1:(365*6+36) )
{  
  colname <- colnames(chain)[i]
  if(grepl("Sigma",colname) ==FALSE )
  {
    next
  }
  samples <- chain[,i]
  SigmaVAR.posterior.modes[colname] <-mlv(samples)$M
}

Sigma.map <-  matrix(unlist(SigmaVAR.posterior.modes),ncol=6, byrow=FALSE)
image(Sigma.map, main = "MAP Sigma")
image(cov.y1,main = "cov of Y1")

b <- unscaled.theta.map[!is.na(Y1)]
c <- Y1[!is.na(Y1)]

plot(b,c)
write.csv(unscaled.theta.map,file = "generativemodel - unscaled-theta-map.csv")
```



