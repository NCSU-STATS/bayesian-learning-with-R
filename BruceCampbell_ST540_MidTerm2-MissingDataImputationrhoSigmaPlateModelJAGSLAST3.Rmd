---
title: "Applied Bayesian Analysis : NCSU ST 540"
subtitle: "Midterm2"
author: "Bruce Campbell"
fontsize: 11pt
output: pdf_document
bibliography: BruceCampbell_ST540_HW_1.bib
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=4)
knitr::opts_chunk$set(fig.width=6)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=38),tidy=TRUE)
library(latex2exp)   
library(pander)
library(ggplot2)
library(GGally)
```

We fit a vector autoregressive  model - $VAR(1) \in \mathbf{R}^6$ given by 

$$y_{t} = \nu + \rho * y_{t-1} + \epsilon$$ 

$$\epsilon \sim N(0,\Sigma)$$

##Data Visualization

```{r,echo=FALSE, results='hide', message=FALSE, warning=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
library(rjags)
library(coda)
library(modeest)
library(MASS)
library(matlib)
load("E2.RData")
N <- nrow(Y1)
p = 6

cov.y1  <- cov(Y1, use = "pairwise.complete.obs")
library("Matrix")
empirical_sigma <- nearPD(cov.y1)$mat

boxplot(Y1)
empirical_precision = inv(as.matrix(empirical_sigma )) #This might be useful as starting value for Wishart prior
image(as.matrix(empirical_sigma),main = c(" Y1 : Nearst PSD to","pairwise complete obs cov matrix"))
#heatmap(as.matrix(empirical_sigma),main = " Heatmap Y1 PSD")

ggplot(data.frame(Y3=Y3) ,aes(x=1:365, y=Y3)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Y3")
ggplot(data.frame(mean.Y1=rowMeans(Y1,na.rm = TRUE)) ,aes(x=1:365, y=mean.Y1)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Row Means Y1")

```

```{r}
library(R2OpenBUGS)
n.chains = 2
n.thin = 2
n.burnin =2000
n.samples =20000

stacks_dat <- list(Y1=Y1,Y2=Y2,Y3=Y3,p = 6,   N = 365, I=diag(p))

mlr_model <- function(){
  
  #VAR(1) parameters <-----THIS IS FIT IN JAGS BELOW !
  # for(i in 2:N) {
  #   Y1pred[i,1:p] ~ dmnorm( nu[i,1:p] ,precisionVar[,])
  #   nu[i,1:p]<-  mu2[1:p] + rhoXtheta[1:p,1:p]    
  #   for ( k in 1: p ){
  #     for ( l in 1: p){
  #       rhoXtheta[k , l ] <- inprod(rho[k , 1: p ] , theta[l, 1: p ])
  #     }
  #   }
  # }
  # 
  # for(i in 1:p) {
  #   mu2[i] ~ dnorm(0, 0.01)
  #   rho[i,i] ~ dunif(-1, 1)
  #   for(j in (i+1):p) {
  #     rho[i,j] ~ dunif(-1,1)
  #     rho[j,i] ~ dunif(-1,1)
  #   }
  # }
  # 
  # precisionVar[1:p,1:p] ~ dwish(I[,],p+1)
  # SigmaVAR[1:p,1:p] <- inverse(precisionVar[,])
  
  #theta
  for(i in 1:N) {
    
    Y2[i] ~ dnorm(theta[i,1], inv.var)
    
    Y3[i] ~ dnorm(thetaBar[i] , inv.var)
    thetaBar[i] <- 1/6 *(theta[i,1]+theta[i,2]+theta[i,3]+theta[i,4]+theta[i,5]+theta[i,6])
  }
    
  inv.var   ~ dgamma(0.01, 0.01)
  for(i in 1:N){
    for(j in 1:p){
      theta[i,j] ~ dnorm(Y1[i,1],inv.var)
    }
  }

  # Missing data model for x
  for(i in 1:N){
    Y1[i,1:p]~dmnorm(Y1_mn[],Y1_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    Y1_mn[j]~dnorm(0,0.01)
  }
  Y1_prec[1:p,1:p]~dwish(R[,],k)
  Y1_cov[1:p,1:p]<-inverse(Y1_prec[,])
  
  k<-p+0.1
  for(j1 in 1:p){for(j2 in 1:p){R[j1,j2]<-0.1*equals(j1,j2)}} #R is diagonal
}

mlr_inits <- function() {
  list(  I = diag(p), tau = 0.01)
}

samps <- bugs(data = stacks_dat, 
            inits = mlr_inits, 
            parameters.to.save = c("theta"), 
            model.file = mlr_model, 
            codaPkg = TRUE,debug=TRUE,
            n.chains = n.chains, n.burnin=n.burnin, n.iter = n.samples,n.thin = n.thin, DIC=F)

out.coda <- read.bugs(samps)
#save(out.coda,file = "out.coda_OPENBUGS_Imputation.RData")

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
 
# imputedY1 <- Y1
# for( i in 1:ncol(chain) )
# { 
#   colname <- colnames(chain)[i]
#   idx <-gsub('x','',colname)
#   idx <-gsub('\\[','',idx)
#   idx<-gsub('\\]','',idx)
#   strsplit(idx,",")
#   idi <- as.numeric(strsplit(idx,",")[[1]][1])
#   idj <- as.numeric(strsplit(idx,",")[[1]][2])
#   
#   if(grepl("Sigma",colname)  )
#   {
#     print(paste("skipping ",colname,sep=""))
#     next
#   }
#   samples <- chain[,i]
#   imputedY1[idi,idj] <-mlv(samples)$M
# }

theta.posterior.modes <- list()
for( i in 1:ncol(chain) )
{ 
 colname <- colnames(chain)[i]
 if(grepl("theta",colname)  )
 {
   samples <- chain[,i]
   theta.posterior.modes[colname]  <-mlv(samples)$M
   
 }else
 {
   next
 }
}
theta.posterior <-  matrix(unlist(theta.posterior.modes),ncol=6, byrow=FALSE)
image(theta.posterior,main = "Estimated theta_{i,j}")
boxplot(theta.posterior)
ggplot(data.frame(mean.Y1=rowMeans(theta.posterior,na.rm = TRUE)) ,aes(x=1:365, y=mean.Y1)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle(" Row Means theta.posterior")
write.csv(theta.posterior,file = "theta.posterior.csv")
```

###JAGS Fit for  VAR(1) parameters

```{r}

theta <- read.csv("theta.posterior.csv")
theta$X <- NULL

library(rjags)
library(coda)

# Jags code to fit the model to the simulated data
model_code = '
model
{
  for(i in 2:N) {
    Y1pred[i,1:p] ~ dmnorm( nu[i,1:p] ,precisionVar[,])
    nu[i,1:p]<-  mu2[1:p] + rho %*%  theta[i-1,]
    }
  
  for(i in 1:p) {
    mu2[i] ~ dnorm(0, 0.01)
    rho[i,i] ~ dunif(-1, 1)
    for(j in (i+1):p) {
      rho[i,j] ~ dunif(-1,1)
      rho[j,i] ~ dunif(-1,1)
    }
  }
  
  precisionVar[1:p,1:p] ~ dwish(I,p+1)
  SigmaVAR ~ dwish(I, p+1)
  
}
'
model_data = list(theta=theta,N = N, p = p, I = diag(p))

model_parameters =  c("rho","SigmaVAR","mu2" )

model <- jags.model(textConnection(model_code),data = model_data,n.chains = n.chains)#Compile Model Graph

update(model, n.burnin, progress.bar="none"); # Burnin

out.coda  <- coda.samples(model, variable.names=model_parameters,n.iter=n.samples) 

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]

SigmaVAR.posterior.modes <- list()
for( i in 1:ncol(chain) )
{ 
 colname <- colnames(chain)[i]
 if(grepl("SigmaVAR",colname)  )
 {
   samples <- chain[,i]
   SigmaVAR.posterior.modes[colname]  <-mlv(samples)$M
   
 }else
 {
   next
 }
}
Sigma.map <-  matrix(unlist(SigmaVAR.posterior.modes),ncol=6, byrow=FALSE)
pander(Sigma.map, caption = "MAP estimate of Sigma2")

```

