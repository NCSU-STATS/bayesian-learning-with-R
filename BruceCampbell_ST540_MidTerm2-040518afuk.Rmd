---
title: "Applied Bayesian Analysis : NCSU ST 540"
subtitle: "Midterm2"
author: "Bruce Campbell"
fontsize: 11pt
output: pdf_document
bibliography: BruceCampbell_ST540_HW_1.bib
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=5)
knitr::opts_chunk$set(fig.width=6)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=38),tidy=TRUE)
library(latex2exp)   
library(pander)
library(ggplot2)
library(GGally)
```



# Test section - VAR(1) in JAGS

This section is a test section where we generate and fit a vector autoregressive  model - $VAR(1) \in \mathbf{R}^6$ given by 

$$y_{t} = \nu + \rho * y_{t-1} + \epsilon$$ 

$$\epsilon \sim N(0,\Sigma)$$

We use the $y1$ data to calculate a NaN firendly sample covariance and then we find the nearest positive semidefinite matrix to use to generate data for the model. 

```{r,echo=FALSE}
library(rjags)
library(coda)
library(modeest)
library(MASS)
load("E2.RData")
cov.y1  <- cov(Y1, use = "pairwise.complete.obs")
heatmap(cov.y1,main = "cov of Y1")
library("Matrix")
sig <- nearPD(cov.y1)
heatmap(as.matrix(sig$mat),main = "cov of y - our test data ")
N = 365
p <-6
Sigma = sig$mat
rho = .7
nu = matrix(rep(.02,6), p, 1)
y = matrix(NA, N,p)
y[1,] = 1:6
for(t in 2:N) 
{
  y[t,] = mvrnorm(1, nu + rho * y[t-1,], Sigma)
}

Y1.test <-y
Y2.test <- y[,1]
Y3.test <- rowMeans(y)

#redaction pproportions
Y1.redaction.prop <- .1
Y2.redaction.prop <- .1
Y3.redaction.prop <- .1

Y1.test.redacted <- Y1.test
Y2.test.redacted <- Y2.test
Y3.test.redacted <- Y3.test

Y1.test.redacted.unlisted <- unlist(Y1.test.redacted)
Y1.NA.index <- sample(length(Y1.test.redacted), floor(length(Y1.test.redacted)* Y1.redaction.prop))
Y1.test.redacted.unlisted[Y1.NA.index] <- NA
Y1.test.redacted<-matrix(Y1.test.redacted.unlisted,ncol=6, byrow=FALSE)

Y2.NA.index <- sample(length(Y2.test.redacted), floor(length(Y2.test.redacted)* Y2.redaction.prop))
Y2.test.redacted[Y2.NA.index] <- NA

Y3.NA.index <- sample(length(Y3.test.redacted), floor(length(Y3.test.redacted)* Y3.redaction.prop))
Y3.test.redacted[Y3.NA.index] <- NA
save(Y1.test,Y2.test,Y3.test,Y1.test.redacted,Y2.test.redacted,Y3.test.redacted,file="TestData.RData")

ggplot(data.frame(Y3.test=Y3.test) ,aes(x=1:365, y=Y3.test)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Y3.test")
ggplot(data.frame(mean.Y1=rowMeans(Y1.test,na.rm = TRUE)) ,aes(x=1:365, y=mean.Y1)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Row Means Y1.test")

# Jags code to fit the model to the simulated data
model_code = '
model
{
  
  #theta[1,1:p ] ~ dmnorm(nu, precisionAR)
  #y2[1] ~ dnorm(theta[1,1],sigmaY)
  #y3[t] ~ dnorm(  theta[t,1]/6 +theta[t,2]/6 +theta[t,3]/6 +theta[t,4]/6 +theta[t,5]/6,sigmaY3 )
  
  # Likelihood
  for (t in 2:N) 
  {
    y[t,1:p ] ~ dmnorm(mu[t,1:p ], precisionAR)
    mu[t, 1:p] <- nu + rho * y[t-1,]
    
    #y2[t] ~ dnorm(theta[t,1],sigmaY)
    #y3[t] ~ dnorm(  theta[t,1]/6 +theta[t,2]/6 +theta[t,3]/6 +theta[t,4]/6 +theta[t,5]/6,sigmaY3 )
    #thetaMean[t] <- theta[t,1]/6 +theta[t,2]/6 +theta[t,3]/6 +theta[t,4]/6 +theta[t,5]/6
  }

  # Priors
  rho ~ dunif(-1, 1)
  sigmaY3 ~ dnorm(0, 0.01)
  sigmaY  ~ dnorm(0, 0.01)

  for(i in 1:p) 
  {
    nu[i] ~ dnorm(0, 0.01)
  }

  precisionAR[1:p,1:p]~dwish(R[,],k)
  Sigma[1:p,1:p] <- inverse(precisionAR) 
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
}
'
# Set up the data
model_data = list(N = N, p = p, y = y, I = diag(p))
# Choose the parameters to watch
model_parameters =  c("Sigma","nu", "rho")
n.chains <-2
model <- jags.model(textConnection(model_code),data = model_data,n.chains = n.chains)#Compile Model Graph
update(model, 1000, progress.bar="none"); # Burnin
if(FALSE)
{ 
  out.coda  <- coda.samples(model, variable.names=model_parameters,n.iter=10000) 
  save(out.coda,file = "out.coda_JAGS_VAR1.RData")
}else{
  load("out.coda_JAGS_VAR1.RData")
}

plot(out.coda )

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  #multivariate - don't use if monitoring highly correlated variables
  #gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)
first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()
for( i in 1:length(colnames(chain)) )
{  
  colname <- colnames(chain)[i]
  samples <- chain[,i]
  posterior.means[colname] <-mean(samples)
  posterior.modes[colname] <-mlv(samples)$M
}

Sigma.map <- matrix(unlist(posterior.means)[1:36],ncol=6, byrow=FALSE)

heatmap(as.matrix(Sigma.map),main = "posterior cov (elementwise  mean) ")

rho.map <- unlist(posterior.means)[43]

nu.map <-  unlist(posterior.means)[37:42]

pander (data.frame(rho=rho, rho.estimated=rho.map), caption = "rho")
```

# Missing data imputation in Openbugs - a simple generative model of $\theta$ with no serial correlation accounted for

Here we try to implement missing data imputation in OpenBugs. We use the data set y1 and fit at multivariate model to the data

$$\theta[t] \sim N(Y1[t], \Sigma)$$ we use BUGS and are imputing the missing data in Y1.

Notes :
- This does not take into account the temporal correlation 
- Openbugs was VERY HARD TO FIND on the internet - their website is down.
- We can not yet port the VAR(1) model toOpenBugs for further refinement.  

```{r, results='hide', message=FALSE, warning=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
load("E2.RData")
library(R2OpenBUGS)
library(rjags)
library(coda)
library(modeest)
N <- nrow(Y1)
p = 6
x <- scale(Y1)

openbugs_model <- function(){
  for(i in 1:N)
  {
    theta[i,1:p] ~ dmnorm(x[i,1:p] ,precision2[,])
  }
  
  # Prior for likelihood parameters: mu2, precision2, rho
  rho  ~  dunif(-1,1)
  
  for(j in 1:p)
  {
    mu2[j]  ~  dnorm(0,0.01)
  }
  
  precision2[1:p,1:p] ~ dwish(R[,],k)
  
  # Missing data model for x
  for(i in 1:N){
    x[i,1:p]~dmnorm(x_mn[],x_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    x_mn[j]~dnorm(0,0.01)
  }
  x_prec[1:p,1:p]~dwish(R[,],k)
  x_cov[1:p,1:p]<-inverse(x_prec[,])
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
}

n.chains = 2
nSamples=10000
stacks_dat <- list(x=x,p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.00) }

if(FALSE){
  samps <- bugs(data = stacks_dat, 
              inits = mlr_inits, 
              parameters.to.save = c("theta"), 
              model.file = openbugs_model, 
              codaPkg = TRUE,
              n.chains = n.chains, n.burnin=2000, n.iter = nSamples, n.thin=10, DIC=F)

out.coda <- read.bugs(samps)
save(out.coda,file = "out.coda_BUGS_MVN.RData")
}else{
  load("out.coda_BUGS_MVN.RData")
}

if(n.chains > 1)
{
  gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(gelman.srf$psrf>1.1)
  count.coeff.gt
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()

for( i in 1:(365*6) )
{  
  colname <- colnames(chain)[i]
  
  samples <- chain[,i]
  
  posterior.means[i] <-mean(samples)
  
  posterior.modes[i] <-mlv(samples)$M
}

plot(posterior.means, posterior.modes)

theta.map <-  matrix(unlist(posterior.means)[1:2190],ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1,na.rm = TRUE)) *  apply(Y1, 2,sd,na.rm = TRUE)

write.csv(unscaled.theta.map,file = "unscaled-theta-map.csv")
hist(unscaled.theta.map[,1])

hist(Y1[,1])
```



# VAR(1) on Y1 with imputation in OpenBugs

Thank goodness for StackOverflow.  I was stuck for two days trying to port the VAR model from JAGS to OpenBugs.
This SO posting helped, but it took far too long to get there. 
https://stackoverflow.com/questions/23968710/openbugs-error-expected-multivariate-node


```{r, results='hide', message=FALSE, warning=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
load("E2.RData")
library(R2OpenBUGS)
library(rjags)
library(coda)
library(modeest)
N <- nrow(Y1)
p = 6

openbugs_model <- function(){

      for(i in 2:N) {
    Y1pred[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
       for(j in 1:p){
         theta[i,j]<-  mu2[j] + rho * Y1[i-1,j]
       }
  }
  
  # Priors
  rho  ~  dunif(-1,1)
  
  for(j in 1:p)
  {
    mu2[j]  ~  dnorm(0,0.01)
  }
  
  precision[1:p,1:p] ~ dwish(R[,],k)
  
  # Missing data model for Y1
  for(i in 1:N){
    Y1[i,1:p]~dmnorm(Y1_mn[],Y1_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    Y1_mn[j]~dnorm(0,0.01)
  }
  Y1_prec[1:p,1:p]~dwish(R[,],k)
  Y1_cov[1:p,1:p]<-inverse(Y1_prec[,])
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
}
n.chains <- 2
nSamples=10000
Y1.scaled <- scale(Y1)
stacks_dat <- list(Y1=Y1.scaled,p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.00) }
parameters.to.save = c("Y1pred","precision")
if(TRUE)
{
  samps <- bugs(data = stacks_dat,inits = mlr_inits, parameters.to.save =parameters.to.save, 
              model.file = openbugs_model, 
              codaPkg = TRUE,
              n.chains = n.chains, n.burnin=1000, n.iter = nSamples, n.thin=10, DIC=F)

out.coda <- read.bugs(samps)
save(out.coda,file = "out.coda_BUGS_VAR.RData")

}else{
  load("out.coda_BUGS_MVN.RData")
}

if(n.chains > 1)
{
  gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(gelman.srf$psrf>1.1)
  count.coeff.gt
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()

for( i in 1:(365*6) )
{  
  colname <- colnames(chain)[i]
  
  samples <- chain[,i]
  
  posterior.means[i] <-mean(samples)
  
  posterior.modes[i] <-mlv(samples)$M
}

plot(posterior.means, posterior.modes)

theta.map <-  matrix(unlist(posterior.means)[1:2190],ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1,na.rm = TRUE)) *  apply(Y1, 2,sd,na.rm = TRUE)

write.csv(unscaled.theta.map,file = "unscaled-theta-map.csv")
hist(unscaled.theta.map[,1])

hist(Y1[,1])
```






# VAR(1) on Y1 with starting values, Y2 and Y3 incorporated imputation in OpenBugs

```{r, results='hide', message=FALSE, warning=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
load("E2.RData")
library(R2OpenBUGS)
library(rjags)
library(coda)
library(modeest)
N <- nrow(Y1)
p = 6

openbugs_model <- function(){
      
  Y1pred[1,1:p] ~ dmnorm( Y1[1,1:p] ,precision[,])
  
  for(i in 2:N) {
    Y1pred[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
    for(j in 1:p){
       theta[i,j]<-  mu2[j] + rho * Y1[i-1,j]
    }
    
    #Y2[i] ~ dnorm(rm[i],sigmaY)
    rm[i] <-theta[i,1]
    
    #Y3[i] ~ dnorm( thetaMean[i],sigmaY3 )
    thetaMean[i] <- theta[i,1]/6 +theta[i,2]/6 +theta[i,3]/6 +theta[i,4]/6 +theta[i,5]/6 +theta[i,6]/6
  }
  
  # Priors
  rho  ~  dunif(-1,1)
  sigmaY  ~  dnorm(0,0.01)
    
  for(j in 1:p)
  {
    mu2[j]  ~  dnorm(0,0.01)
  }
  
  precision[1:p,1:p] ~ dwish(R[,],k)
  
  # Missing data model for Y1
  for(i in 1:N){
    Y1[i,1:p]~dmnorm(Y1_mn[],Y1_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    Y1_mn[j]~dnorm(0,0.01)
  }
  Y1_prec[1:p,1:p]~dwish(R[,],k)
  Y1_cov[1:p,1:p]<-inverse(Y1_prec[,])
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
}

Y1.scaled <- scale(Y1)
Y2.scaled <- scale(Y2)
Y3.scaled <- scale(Y3)
stacks_dat <- list(Y1=Y1.scaled,Y2=Y2.scaled, p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.00) }
n.chains <- 2
parameters.to.save = c("Y1pred","precision")
if(TRUE)
{
  samps <- bugs(data = stacks_dat,inits = mlr_inits, parameters.to.save =parameters.to.save, 
              model.file = openbugs_model, 
              codaPkg = TRUE,
              n.chains = 1, n.burnin=50, n.iter = 200, DIC=F)#, n.thin=10

  out.coda <- read.bugs(samps)
save(out.coda,file = "out.coda_BUGS_VAR_with y2y3.RData")
 
 }else{
   load("out.coda_BUGS_VAR_with y2y3.RData")
 }

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  #multivariate - don't use if monitoring highly correlated variables
  #gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()

for( i in 1:(365*6) )
{  
  colname <- colnames(chain)[i]
  
  samples <- chain[,i]
  
  posterior.means[i] <-mean(samples)
  
  posterior.modes[i] <-mlv(samples)$M
}

plot(posterior.means, posterior.modes)

theta.map <-  matrix(unlist(posterior.means)[1:2190],ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1,na.rm = TRUE)) *  apply(Y1, 2,sd,na.rm = TRUE)

write.csv(unscaled.theta.map,file = "unscaled-theta-map.csv")
hist(unscaled.theta.map[,1])

hist(Y1[,1])
```


# TestData.RData VAR(1) on Y1 with starting values, Y2 and Y3 incorporated imputation in OpenBugs
```{r, results='hide', message=FALSE, warning=FALSE}
rm(list = ls())

setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
load("TestData.RData")
library(R2OpenBUGS)
library(rjags)
library(coda)
library(modeest)
N <- nrow(Y1.test.redacted)
p = 6

openbugs_model <- function(){
      
  Y1pred[1,1:p] ~ dmnorm( Y1[1,1:p] ,precision[,])
  #Y1pred[1,1:p] <-  Y1[1,1:p] #Experimental
  
  for(i in 2:N) {
    Y1pred[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
    for(j in 1:p){
       theta[i,j]<-  mu2[j] + rho * Y1[i-1,j]
    }
    
    #Y2[i] ~ dnorm(rm[i],sigmaY)
    rm[i] <-theta[i,1]
    
    #Y3[i] ~ dnorm( thetaMean[i],sigmaY3 )
    thetaMean[i] <- theta[i,1]/6 +theta[i,2]/6 +theta[i,3]/6 +theta[i,4]/6 +theta[i,5]/6 +theta[i,6]/6
  }
  
  # Priors
  rho  ~  dunif(-1,1)
  sigmaY  ~  dnorm(0,0.001)
    
  for(j in 1:p)
  {
    mu2[j]  ~  dnorm(0,0.001)
  }
  
  precision[1:p,1:p] ~ dwish(R[,],k)
  
  #Sigma <- inverse(precision)
  
  # Missing data model for Y1
  for(i in 1:N){
    Y1[i,1:p]~dmnorm(Y1_mn[],Y1_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    Y1_mn[j]~dnorm(0,0.001)
  }
  Y1_prec[1:p,1:p]~dwish(R[,],k)
  Y1_cov[1:p,1:p]<-inverse(Y1_prec[,])
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
  
  Sigma <- inverse(precision[,])
}



openbugs_model_test <- function(){
       
  Y1[1,1:p] ~ dmnorm( Y1[1,1:p] ,precision[,])
  #Y1pred[1,1:p] <-  Y1[1,1:p] #Experimental
  
  for(i in 2:N) {
    Y1[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
    for(j in 1:p){
       theta[i,j]<-  mu2[j] + rho * Y1[i-1,j]
    }
    
    #Y2[i] ~ dnorm(rm[i],sigmaY)
    rm[i] <-theta[i,1]
    
    #Y3[i] ~ dnorm( thetaMean[i],sigmaY3 )
    thetaMean[i] <- theta[i,1]/6 +theta[i,2]/6 +theta[i,3]/6 +theta[i,4]/6 +theta[i,5]/6 +theta[i,6]/6
  }
  
  # Priors
  rho  ~  dunif(0,1)
  sigmaY  ~  dnorm(0,0.01)
    
  for(j in 1:p)
  {
    mu2[j]  ~  dnorm(j,0.01)
  }
  
  precision[1:p,1:p] ~ dwish(R[,],k)
  
  #Sigma <- inverse(precision)
  
  # Missing data model for Y1
  #for(i in 1:N){
    #Y1[i,1:p]~dmnorm(Y1_mn[],Y1_prec[,])
  #}
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    Y1_mn[j]~dnorm(0,0.01)
  }
  Y1_prec[1:p,1:p]~dwish(R[,],k)
  Y1_cov[1:p,1:p]<-inverse(Y1_prec[,])
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
  
  Sigma[1:p,1:p] <- inverse(precision[,])

}



Y1.scaled <- scale(Y1.test)
Y2.scaled <- scale(Y2.test.redacted)
Y3.scaled <- scale(Y3.test.redacted)
stacks_dat <- list(Y1=Y1.test, p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.00) }
n.chains <- 1
parameters.to.save = c("rho","Sigma")
if(TRUE)
{
  samps <- bugs(data = stacks_dat,inits = mlr_inits, parameters.to.save =parameters.to.save, 
              model.file = openbugs_model_test, 
              codaPkg = TRUE,
              n.chains = 2, n.burnin=10000, n.iter = 20000, DIC=F)#, n.thin=10

  out.coda <- read.bugs(samps)
  save(out.coda,file = "out.coda_test.data_BUGS_VAR_with y2y3.RData")
 
 }else{
   load("out.coda_test.data_BUGS_VAR_with y2y3.RData")
 }

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  #multivariate - don't use if monitoring highly correlated variables
  #gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()

for( i in 1:(365*6) )
{  
  colname <- colnames(chain)[i]
  
  samples <- chain[,i]
  
  posterior.means[i] <-mean(samples)
  
  posterior.modes[i] <-mlv(samples)$M
}

plot(posterior.means, posterior.modes)

theta.map <-  matrix(unlist(posterior.means)[1:2190],ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1.test.redacted,na.rm = TRUE)) *  apply(Y1.test.redacted, 2,sd,na.rm = TRUE)

write.csv(unscaled.theta.map,file = "TestData-unscaled-theta-map.csv")


ggplot(data.frame(mean.theta.map=rowMeans(unscaled.theta.map,na.rm = TRUE)) ,aes(x=1:365, y=mean.theta.map)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Row Means theta.map")



plot(theta.map[,1],Y1.test[,1])
plot(theta.map[,2],Y1.test[,2])
plot(theta.map[,3],Y1.test[,3])
plot(theta.map[,4],Y1.test[,4])
plot(theta.map[,5],Y1.test[,5])
plot(theta.map[,5],Y1.test[,6])


plot(unscaled.theta.map[,1],Y1.test[,1])
plot(unscaled.theta.map[,2],Y1.test[,2])
plot(unscaled.theta.map[,3],Y1.test[,3])
plot(unscaled.theta.map[,4],Y1.test[,4])
plot(unscaled.theta.map[,5],Y1.test[,5])

hist(unscaled.theta.map[,1])
hist(Y1.test[,1])

hist(unscaled.theta.map[,1])
hist(Y1.test[,1])

hist(unscaled.theta.map[,2])
hist(Y1.test[,2])

hist(unscaled.theta.map[,3])
hist(Y1.test[,3])

hist(unscaled.theta.map[,4])
hist(Y1.test[,4])

hist(unscaled.theta.map[,5])
hist(Y1.test[,5])

hist(unscaled.theta.map[,6])
hist(Y1.test[,6])

rho.start <-1
colname <- colnames(chain)[rho.start]
samples <- chain[,rho.start]
posterior.means[rho.start] <-mean(samples)
posterior.modes[rho.start] <-mlv(samples)$M
plot(samples)

#Look at precision

posterior.means <- list()
posterior.modes <- list()
for( i in (365*6+1):365*6+1+36 )
{  
  colname <- colnames(chain)[i]
  
  samples <- chain[,i]
  
  posterior.means[i] <-mean(samples)
  
  posterior.modes[i] <-mlv(samples)$M
}
```
