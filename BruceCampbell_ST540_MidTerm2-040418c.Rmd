---
title: "Applied Bayesian Analysis : NCSU ST 540"
subtitle: "Midterm2"
author: "Bruce Campbell"
fontsize: 11pt
output: pdf_document
bibliography: BruceCampbell_ST540_HW_1.bib
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=5)
knitr::opts_chunk$set(fig.width=6)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=38),tidy=TRUE)
library(latex2exp)   
library(pander)
library(ggplot2)
library(GGally)
```



# Test section - VAR(1) in JAGS

This section is a test section where we generate and fit a vector autoregressive  model - $VAR(1) \in \mathbf{R}^6$ given by 

$$y_{t} = \nu + \rho * y_{t-1} + \epsilon$$ 

$$\epsilon \sim N(0,\Sigma)$$
# VAR(1) on Y1 with starting values, Y2 and Y3 incorporated imputation in OpenBugs

```{r, results='hide', message=FALSE, warning=FALSE}
rm(list = ls())
setwd("c:/e/brucebcampbell-git/bayesian-learning-with-R")
load("E2.RData")
library(R2OpenBUGS)
library(rjags)
library(coda)
library(modeest)
N <- nrow(Y1)
p = 6

mlr_model2 <- function(){
  
      Y1pred[1,1:p] ~ dmnorm( Y1[1,1:p] ,precision[,])
  
      for(i in 2:N) {
    Y1pred[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
        for(j in 1:p){
         theta[i,j]<-  mu2[j] + rho * Y1[i-1,j]
        }
        #Y2[i] ~ dnorm(rm[i],sigmaY)
        rm[i] <-theta[i,1]
        #Y3[i] ~ dnorm( thetaMean[i],sigmaY3 )
        thetaMean[i] <- theta[i,1]/6 +theta[i,2]/6 +theta[i,3]/6 +theta[i,4]/6 +theta[i,5]/6 +theta[i,6]/6
        }
  
  # Priors
  rho  ~  dunif(-1,1)
  sigmaY  ~  dnorm(0,0.01)
    
  for(j in 1:p)
  {
    mu2[j]  ~  dnorm(0,0.01)
  }
  
  precision[1:p,1:p] ~ dwish(R[,],k)
  
  # Missing data model for Y1
  for(i in 1:N){
    Y1[i,1:p]~dmnorm(Y1_mn[],Y1_prec[,])
  }
  
  # Priors for missing-data model parameters
  for(j in 1:p){
    Y1_mn[j]~dnorm(0,0.01)
  }
  Y1_prec[1:p,1:p]~dwish(R[,],k)
  Y1_cov[1:p,1:p]<-inverse(Y1_prec[,])
  
  k <- p+0.1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- 0.1*equals(j1,j2)
    }
  }
}

Y1.scaled <- scale(Y1)
Y2.scaled <- scale(Y2)
Y3.scaled <- scale(Y3)
stacks_dat <- list(Y1=Y1.scaled,Y2=Y2.scaled, p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.00) }
parameters.to.save = c("Y1pred","precision")
#if(TRUE)
#{
  samps <- bugs(data = stacks_dat,inits = mlr_inits, parameters.to.save =parameters.to.save, 
              model.file = mlr_model2, 
              codaPkg = TRUE,
              n.chains = 1, n.burnin=50, n.iter = 200, DIC=F)#, n.thin=10
  out.coda <- read.bugs(samps)
  save(out.coda,file = "out.coda_BUGS_VAR.RData")
# 
# }else{
#   load("out.coda_BUGS_MVN.RData")
# }

if(n.chains > 1)
{
  gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(gelman.srf$psrf>1.1)
  count.coeff.gt
}

chains.ess <- lapply(out.coda,effectiveSize)

first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()

for( i in 1:(365*6) )
{  
  colname <- colnames(chain)[i]
  
  samples <- chain[,i]
  
  posterior.means[i] <-mean(samples)
  
  posterior.modes[i] <-mlv(samples)$M
}

plot(posterior.means, posterior.modes)

theta.map <-  matrix(unlist(posterior.means)[1:2190],ncol=6, byrow=FALSE)

unscaled.theta.map <- ( theta.map +colMeans(Y1,na.rm = TRUE)) *  apply(Y1, 2,sd,na.rm = TRUE)

write.csv(unscaled.theta.map,file = "unscaled-theta-map.csv")
hist(unscaled.theta.map[,1])

hist(Y1[,1])
```
