---
title: "Applied Bayesian Analysis : NCSU ST 540"
subtitle: "Midterm2"
author: "Bruce Campbell"
fontsize: 11pt
output: pdf_document
bibliography: BruceCampbell_ST540_HW_1.bib
---

---
```{r setup, include=FALSE,echo=FALSE}
rm(list = ls())
setwd("d:/brucebcampbell-git/bayesian-learning-with-R")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev = 'pdf')
knitr::opts_chunk$set(cache=TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(prompt=FALSE)
knitr::opts_chunk$set(fig.height=4)
knitr::opts_chunk$set(fig.width=5)
knitr::opts_chunk$set(warning=FALSE)
knitr::opts_chunk$set(message=FALSE)
knitr::opts_knit$set(root.dir = ".")
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=38),tidy=TRUE)
library(latex2exp)   
library(pander)
library(ggplot2)
library(GGally)
```



# Test section - VAR(1) in JAGS

This section is a test section where we generate and fit a vector autoregressive  model - $VAR(1) \in \mathbf{R}^6$ given by 

$$y_{t} = \nu + \rho * y_{t-1} + \epsilon$$ 

$$\epsilon \sim N(0,\Sigma)$$

We use the $y1$ data to calculate a NaN firendly sample covariance and then we find the nearest positive semidefinite matrix to use to generate data for the model. 

Notes
- imputation is not working for this model
- this section seeks to find the parameters that best explain the data

```{r}
library(rjags)
library(coda)
library(modeest)
library(MASS)
load("E2.RData")

nSamples <- 10000
n.chains <- 2

cor.y1 <- cor(Y1, use = "pairwise.complete.obs")
cov.y1  <- cov(Y1, use = "pairwise.complete.obs")
heatmap(cor.y1,main = "corr of Y1")
ggplot(data.frame(Y3=Y3) ,aes(x=1:365, y=Y3)) + geom_point(alpha=0.25) + geom_smooth( method="loess", span=0.22) +ggtitle("Y3")
```


```{r}
N <- nrow(Y1)
p = 6
Y1.scaled <- scale(Y1)
# Try to simulate using the corr
#install.packages("Matrix")
library("Matrix")

sig <- nearPD(cov.y1)
N = 365
Sigma = sig$mat 
rho = .6
nu = matrix(rep(.1,6), p, 1)
y = matrix(NA, N,p)
y[1,] = nu
for(t in 2:N) 
{
  y[t,] = mvrnorm(1, nu + rho * y[t-1,], Sigma)
}

# Jags code to fit the model to the simulated data
model_code = '
model
{
  # Likelihood
  for (t in 2:N) 
  {
    y[t, ] ~ dmnorm(mu[t, ], precisionAR)
    mu[t, 1:p] <- nu + rho * y[t-1,]
  }
  precisionAR ~ dwish(I, p+1)
  Sigma <- inverse(precisionAR)  
  
  # Priors
  rho ~ dunif(-1, 1)
  for(i in 1:p) 
  {
    nu[i] ~ dnorm(0, 0.01)
  }
}
'
# Set up the data
model_data = list(N = N, p = p, y = y, I = diag(p))
# Choose the parameters to watch
model_parameters =  c("Sigma","nu", "rho" )

model <- jags.model(textConnection(model_code),data = model_data,n.chains = n.chains)#Compile Model Graph
update(model, nSamples, progress.bar="none"); # Burnin
if(TRUE)
{ 
  out.coda  <- coda.samples(model, variable.names=model_parameters,n.iter=2*nSamples) 
  save(out.coda,file = "out.coda_JAGS_VAR1.RData")
}else{
  load("out.coda_JAGS_VAR1.RData")
}

#plot(out.coda )

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  #multivariate - don't use if monitoring highly correlated variables
  #gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)
first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()
for( i in 1:length(colnames(chain)) )
{  
  colname <- colnames(chain)[i]
  plot(chain[,i])
  samples <- chain[,i]
  posterior.means[colname] <-mean(samples)
  posterior.modes[colname] <-mlv(samples)$M
}

Sigma.map <- unlist(posterior.means)[1:36]
Sigma.model<- unlist(sig$mat)
plot(Sigma.map,Sigma.model)
rho.map <- unlist(posterior.means)[43]

nu.map <-  unlist(posterior.means)[37:42]

```
# Replicate JAGS in OpenBugs

# VAR(1) on Y1 with starting values, Y2 and Y3 incorporated imputation in OpenBugs

```{r, results='hide', message=FALSE, warning=FALSE}
library(R2OpenBUGS)

openbugs_model <- function(){
  for(i in 2:N) {
    Y1[i,1:p] ~ dmnorm( theta[i,1:p] ,precision[,])
    for(j in 1:p){
      theta[i,j]<-  nu[j] + rho * Y1[i-1,j]
    }
  }
  
  # Priors
  rho  ~  dunif(-1,1)

  for(j in 1:p)
  {
    nu[j]  ~  dnorm(0,0.01)
  }
  
  #Sigma[1:p,1:p] <- inverse(precision[1:p,1:p])
  
  precision[1:p,1:p] ~ dwish(R[,],k)

  
  k <- p+1
  for(j1 in 1:p)
  {
    for(j2 in 1:p)
    {
      R[j1,j2] <- equals(j1,j2)
    }
  }
  
}

stacks_dat <- list(Y1=scale(y), p = 6,   N = 365)
mlr_inits <- function() {   list( rho = 0.00 , precision = diag(p)) }
parameters.to.save = c("precision","nu" ,"rho")

samps <- bugs(data = stacks_dat,inits = mlr_inits, parameters.to.save =parameters.to.save, 
            model.file = openbugs_model, 
            codaPkg = TRUE,
            n.chains = n.chains, n.burnin=10000, n.iter = 2*nSamples, DIC=F)#, n.thin=10
out.coda <- read.bugs(samps)
save(out.coda,file = "out.coda_BUGS_VAR.RData")

if(n.chains > 1)
{
  g <- matrix(NA, nrow=nvar(out.coda), ncol=2)
  for (v in 1:nvar(out.coda)) {
   g[v,] <- gelman.diag(out.coda[,v])$psrf
  }
  #multivariate - don't use if monitoring highly correlated variables
  #gelman.srf <-gelman.diag(out.coda)
  count.coeff.gt <- sum(g[,1]>1.1)
  count.coeff.gt
  plot(g[,1],main="Gelman-Rubin ")
}

chains.ess <- lapply(out.coda,effectiveSize)
first.chain.ess <- chains.ess[1]
plot(unlist(first.chain.ess), main="Effective Sample Size")

chain <- out.coda[[1]]
posterior.means <- list()
posterior.modes <- list()
for( i in 1:length(colnames(chain)) )
{  
  colname <- colnames(chain)[i]
  plot(chain[,i])
  samples <- chain[,i]
  posterior.means[colname] <-mean(samples)
  posterior.modes[colname] <-mlv(samples)$M
}

Sigma.map <- unlist(posterior.means)[1:36]
Sigma.model<- unlist(sig$mat)
plot(Sigma.map,Sigma.model)
rho.map <- unlist(posterior.means)[43]

nu.map <-  unlist(posterior.means)[37:42]

```
